import torch
import coremltools as ct
from transformers import AutoTokenizer, OPTForCausalLM

# ------------------------------------------------
# 1Ô∏è‚É£ Charger le mod√®le pr√©-entra√Æn√©
# ------------------------------------------------
model_name = "facebook/opt-350m"  # Un bon compromis taille/performance
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = OPTForCausalLM.from_pretrained(model_name)
model.eval()

print("‚úÖ Mod√®le et tokenizer charg√©s depuis :", model_name)

# ------------------------------------------------
# 2Ô∏è‚É£ Cr√©er un wrapper simplifi√© pour TorchScript
# ------------------------------------------------
class OPTWrapper(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.config = model.config
        self.device = next(model.parameters()).device

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):
        # Assurer que les tenseurs sont sur le bon device et ont le bon type
        input_ids = input_ids.to(self.device)
        attention_mask = attention_mask.to(self.device)
        
        # Forcer le type √† long pour √™tre compatible avec le mod√®le
        input_ids = input_ids.long()
        attention_mask = attention_mask.long()
        
        # Forward pass avec param√®tres explicites et minimaux
        with torch.no_grad():
            outputs = self.model(
                input_ids,
                attention_mask=attention_mask,
                use_cache=False,
                output_attentions=False,
                output_hidden_states=False,
                return_dict=True
            )
        return outputs.logits


wrapped_model = OPTWrapper(model)

# ------------------------------------------------
# 3Ô∏è‚É£ Exemple d'entr√©e pour tra√ßage
# ------------------------------------------------
example_text = "Patient: I have a cough and fever."
example = tokenizer(
    example_text,
    return_tensors="pt",
    padding="max_length",
    max_length=64,
    truncation=True
)

# Force les tenseurs √† √™tre sur CPU et en mode eval
input_ids = example["input_ids"].cpu()
attention_mask = example["attention_mask"].cpu()

print(f"‚úì Taille des tenseurs d'entr√©e :", input_ids.shape)

# ------------------------------------------------
# 4Ô∏è‚É£ Conversion CoreML avec s√©quence dynamique
# ------------------------------------------------
# Tracer le mod√®le avec des entr√©es sp√©cifiques
with torch.no_grad():
    traced_model = torch.jit.trace(
        wrapped_model,
        (input_ids, attention_mask)
    )
mlmodel = ct.convert(
    traced_model,
    convert_to="mlprogram",
    inputs=[
        ct.TensorType(name="input_ids",
                      shape=(1, ct.RangeDim(1, 512))),  # min=1, max=512 tokens
        ct.TensorType(name="attention_mask",
                      shape=(1, ct.RangeDim(1, 512)))
    ],
    minimum_deployment_target=ct.target.iOS15,
)

# ------------------------------------------------
# 5Ô∏è‚É£ Sauvegarde
# ------------------------------------------------
output_path = "model_export/MedicalLLM.mlpackage"
mlmodel.save(output_path)

print(f"‚úÖ Conversion termin√©e avec succ√®s !\nüì¶ Mod√®le export√© vers : {output_path}")
