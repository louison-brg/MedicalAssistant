"""
prepare_professor_dataset_phi3.py
---------------------------------
Cr√©e un dataset ‚ÄúStudent‚ÄìProfessor‚Äù pour le fine-tuning du mod√®le Phi-3-mini.
Sources :
 - MedQA (USMLE)
 - Textbooks m√©dicaux anglais (.txt)
 - MedDialog (anglais)
"""

import os
from datasets import load_dataset, concatenate_datasets, Dataset
from transformers import AutoTokenizer
from tqdm import tqdm

# ==========================================================
# ‚öôÔ∏è Configuration
# ==========================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SAVE_DIR = os.path.join(BASE_DIR, "processed_professor_phi3")
RAW_DIR = os.path.join(BASE_DIR, "raw")

TOKENIZER_MODEL = "microsoft/phi-3-mini-4k-instruct"
MAX_LENGTH = 512

os.makedirs(SAVE_DIR, exist_ok=True)

print("üöÄ Cr√©ation du dataset Prof‚Äì√âtudiant pour Phi-3...\n")

# Initialisation du tokenizer
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})

print(f"‚úÖ Tokenizer charg√© : {TOKENIZER_MODEL}\n")

# ==========================================================
# 1Ô∏è‚É£ MedQA (USMLE)
# ==========================================================
def format_medqa(example):
    q = example.get("question", "")
    opts = example.get("options", {})
    ans = example.get("answer", "")
    meta = example.get("meta_info", "")

    if isinstance(opts, dict):
        options_text = "\n".join([f"{k}. {v}" for k, v in opts.items()])
    else:
        options_text = str(opts)

    text = (
        f"Student: {q}\n"
        f"Options:\n{options_text}\n"
        f"Professor: The correct answer is {ans}. "
        f"Explanation: {meta if meta else 'This involves physiological and pharmacological reasoning.'}"
    )
    return {"text": text}

print("üìò Chargement de MedQA (USMLE)...")
MEDQA_PATH = os.path.join(RAW_DIR, "med_qa/data_clean/data_clean/questions/US/train.jsonl")
medqa = load_dataset("json", data_files=MEDQA_PATH)["train"].map(format_medqa)
print(f"‚úÖ MedQA format√© : {len(medqa)} exemples\n")

# ==========================================================
# 2Ô∏è‚É£ Textbooks anglais (.txt)
# ==========================================================
print("üìö Chargement des textbooks anglais...")
TEXTBOOK_DIR = os.path.join(RAW_DIR, "med_qa/data_clean/data_clean/textbooks/en")
text_data = []

for filename in os.listdir(TEXTBOOK_DIR):
    if filename.endswith(".txt"):
        path = os.path.join(TEXTBOOK_DIR, filename)
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read().strip()
            if len(content) > 200:
                title = os.path.splitext(filename)[0].replace("_", " ").title()
                text_data.append({
                    "text": f"Student: Can you explain the topic of {title}?\nProfessor: {content}"
                })

textbooks = Dataset.from_list(text_data)
print(f"‚úÖ Textbooks format√©s : {len(textbooks)} exemples\n")

# ==========================================================
# 3Ô∏è‚É£ MedDialog (anglais)
# ==========================================================
print("üí¨ Chargement de MedDialog...")

def format_meddialog(example):
    desc = example.get("description", "")
    utts = example.get("utterances", [])
    dialogue = " ".join(utts).replace("patient:", "student:").replace("doctor:", "professor:")
    text = f"Case: {desc}\n{dialogue}"
    return {"text": text}

MEDDIALOG_PATH = os.path.join(BASE_DIR, "processed/english-train.json")
meddialog = load_dataset("json", data_files=MEDDIALOG_PATH)["train"].map(format_meddialog)
print(f"‚úÖ MedDialog format√© : {len(meddialog)} exemples\n")

# ==========================================================
# 4Ô∏è‚É£ Fusion
# ==========================================================
print("üß© Fusion de tous les datasets...")
combined = concatenate_datasets([medqa, textbooks, meddialog])
print(f"‚úÖ Total : {len(combined)} exemples combin√©s\n")

# ==========================================================
# 5Ô∏è‚É£ Sauvegarde non-tokenis√©e
# ==========================================================
RAW_SAVE_PATH = os.path.join(SAVE_DIR, "raw_text_dataset")
combined.save_to_disk(RAW_SAVE_PATH)
print(f"üíæ Dataset texte sauvegard√© : {RAW_SAVE_PATH}\n")

# ==========================================================
# 6Ô∏è‚É£ Tokenisation
# ==========================================================
def tokenize_function(example):
    tokens = tokenizer(example["text"], truncation=False)
    input_ids = tokens["input_ids"]
    result_input_ids, result_attention_masks = [], []

    for i in range(0, len(input_ids), MAX_LENGTH):
        chunk = input_ids[i:i + MAX_LENGTH]
        attention_mask = [1] * len(chunk)

        if len(chunk) < MAX_LENGTH:
            pad_len = MAX_LENGTH - len(chunk)
            chunk += [tokenizer.pad_token_id] * pad_len
            attention_mask += [0] * pad_len

        result_input_ids.append(chunk)
        result_attention_masks.append(attention_mask)

    return {"input_ids": result_input_ids, "attention_mask": result_attention_masks}

print("üî† Tokenisation avec Phi-3 tokenizer...")
temp_dataset = combined.map(tokenize_function, batched=False, remove_columns=combined.column_names)

flat_input_ids, flat_attention_masks = [], []
for ex in temp_dataset:
    for i in range(len(ex["input_ids"])):
        flat_input_ids.append(ex["input_ids"][i])
        flat_attention_masks.append(ex["attention_mask"][i])

tokenized_dataset = Dataset.from_dict({
    "input_ids": flat_input_ids,
    "attention_mask": flat_attention_masks
})

print(f"‚úÖ {len(tokenized_dataset):,} s√©quences pr√™tes pour l‚Äôentra√Ænement")

# ==========================================================
# 7Ô∏è‚É£ Sauvegarde finale
# ==========================================================
TOKENIZED_PATH = os.path.join(SAVE_DIR, "tokenized")
tokenized_dataset.save_to_disk(TOKENIZED_PATH)
print(f"üéâ Dataset tokenis√© sauvegard√© dans : {TOKENIZED_PATH}\n")
