"""
prepare_datasets.py
-------------------
Fusionne et tokenise les datasets :
- MedQA (USMLE)
- Textbooks m√©dicaux anglais (.txt)
- MedDialog (english-train.json)

Sortie :
Dataset fusionn√©, nettoy√© et tokenis√© sauvegard√© dans training/data/processed/
"""

import os
from datasets import load_dataset, concatenate_datasets, Dataset
from transformers import AutoTokenizer
from tqdm import tqdm

# =======================================================================
# 0Ô∏è‚É£ Configuration de base
# =======================================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SAVE_DIR = os.path.join(BASE_DIR, "processed")
TOKENIZER_MODEL = "gpt2"
MAX_LENGTH = 512

os.makedirs(SAVE_DIR, exist_ok=True)

# Initialisation du tokenizer
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

print("üöÄ Pr√©paration des datasets m√©dicaux...\n")


# =======================================================================
# 1Ô∏è‚É£ MedQA (USMLE)
# =======================================================================
def format_medqa(example):
    q = example.get("question", "")
    opts = example.get("options", {})
    ans = example.get("answer", "")
    meta = example.get("meta_info", "")

    if isinstance(opts, dict):
        options_text = "\n".join([f"{k}. {v}" for k, v in opts.items()])
    else:
        options_text = str(opts)

    text = (
        f"Question: {q}\n"
        f"Options:\n{options_text}\n"
        f"Answer: {ans}\n"
        f"Metadata: {meta}"
    )
    return {"text": text}


print("üìò Chargement de MedQA (USMLE)...")

MEDQA_PATH = os.path.join(BASE_DIR, "raw/med_qa/data_clean/data_clean/questions/US/train.jsonl")

medqa = load_dataset("json", data_files=MEDQA_PATH)["train"].map(format_medqa)

print(f"‚úÖ MedQA: {len(medqa)} exemples charg√©s et format√©s\n")


# =======================================================================
# 2Ô∏è‚É£ Textbooks anglais (.txt)
# =======================================================================
print("üìö Chargement des textbooks anglais...")

TEXTBOOK_DIR = os.path.join(BASE_DIR, "raw/med_qa/data_clean/data_clean/textbooks/en")
text_data = []

for filename in os.listdir(TEXTBOOK_DIR):
    if filename.endswith(".txt"):
        path = os.path.join(TEXTBOOK_DIR, filename)
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read().strip()
            if len(content) > 200:
                text_data.append({"text": content})

textbooks = Dataset.from_list(text_data)

print(f"‚úÖ Textbooks: {len(textbooks)} documents charg√©s\n")


# =======================================================================
# 3Ô∏è‚É£ MedDialog (anglais)
# =======================================================================
print("üí¨ Chargement de MedDialog (anglais)...")

def format_meddialog(example):
    desc = example.get("description", "")
    utts = example.get("utterances", [])
    # Les utterances sont d√©j√† sous forme de texte ("patient: ..." / "doctor: ...")
    dialogue = " ".join(utts)
    text = f"Case: {desc}\nDialogue: {dialogue}"
    return {"text": text}

MEDDIALOG_PATH = os.path.join(BASE_DIR, "processed/english-train.json")

meddialog = load_dataset("json", data_files=MEDDIALOG_PATH)["train"].map(format_meddialog)

print(f"‚úÖ MedDialog: {len(meddialog)} dialogues charg√©s\n")


# =======================================================================
# 4Ô∏è‚É£ Fusion des datasets
# =======================================================================
print("üß© Fusion des datasets...")
combined = concatenate_datasets([medqa, textbooks, meddialog])
print(f"‚úÖ Total: {len(combined)} exemples combin√©s\n")


# =======================================================================
# 5Ô∏è‚É£ Tokenisation
# =======================================================================
def tokenize_function(example):
    # Tokenisation sans troncature globale
    tokens = tokenizer(example["text"], truncation=False)
    input_ids = tokens["input_ids"]

    result_input_ids = []
    result_attention_masks = []

    # D√©coupage en morceaux de 512 tokens
    for i in range(0, len(input_ids), MAX_LENGTH):
        chunk = input_ids[i:i + MAX_LENGTH]
        attention_mask = [1] * len(chunk)

        # Padding si besoin
        if len(chunk) < MAX_LENGTH:
            pad_len = MAX_LENGTH - len(chunk)
            chunk += [tokenizer.pad_token_id] * pad_len
            attention_mask += [0] * pad_len

        result_input_ids.append(chunk)
        result_attention_masks.append(attention_mask)

    return {
        "input_ids": result_input_ids,
        "attention_mask": result_attention_masks
    }



print("üî† Tokenisation en cours...")
temp_dataset = combined.map(tokenize_function, batched=False, remove_columns=combined.column_names)

# =====================================================
# üß© Flatten manuel des sous-listes (chunks)
# =====================================================
from datasets import Dataset

print("üîß Flatten des s√©quences multiples par document...")
flat_input_ids = []
flat_attention_masks = []

for ex in temp_dataset:

    if isinstance(ex["input_ids"][0], list):
        for i in range(len(ex["input_ids"])):
            flat_input_ids.append(ex["input_ids"][i])
            flat_attention_masks.append(ex["attention_mask"][i])
    else:
        flat_input_ids.append(ex["input_ids"])
        flat_attention_masks.append(ex["attention_mask"])

tokenized_dataset = Dataset.from_dict({
    "input_ids": flat_input_ids,
    "attention_mask": flat_attention_masks
})

print(f"‚úÖ Dataset aplati : {len(tokenized_dataset):,} s√©quences pr√™tes √† l‚Äôentra√Ænement")


# =======================================================================
# 6Ô∏è‚É£ Sauvegarde finale
# =======================================================================
print("üíæ Sauvegarde du dataset tokeniz√©...")
tokenized_dataset.save_to_disk(SAVE_DIR)

print(f"üéâ Dataset final pr√™t pour l'entra√Ænement !\nüìÇ Emplacement : {SAVE_DIR}")
