"""
prepare_professor_dataset_mlx_final.py
--------------------------------------
CrÃ©e un dataset â€œStudentâ€“Professorâ€ propre et compatible MLX :
- Fusionne MedQA, Textbooks et MedDialog
- Nettoie et vÃ©rifie les exemples
- Split en train / eval
- Sauvegarde en JSONL (MLX-ready)
"""

import os
import json
import random
from datasets import load_dataset, concatenate_datasets, Dataset

# ==========================================================
# âš™ï¸ Configuration
# ==========================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SAVE_DIR = os.path.join(BASE_DIR, "mlx_data")
RAW_DIR = os.path.join(BASE_DIR, "raw")

os.makedirs(SAVE_DIR, exist_ok=True)

TRAIN_RATIO = 0.9  # 90% train, 10% eval
MIN_TEXT_LEN = 50  # Ã©viter les dialogues trop courts

print("ğŸš€ CrÃ©ation du dataset Profâ€“Ã‰tudiant (format MLX)...\n")

# ==========================================================
# 1ï¸âƒ£ MedQA (USMLE)
# ==========================================================
def format_medqa(example):
    q = example.get("question", "").strip()
    opts = example.get("options", {})
    ans = example.get("answer", "")
    meta = example.get("meta_info", "")

    if isinstance(opts, dict):
        options_text = "\n".join([f"{k}. {v}" for k, v in opts.items()])
    else:
        options_text = str(opts)

    text = (
        f"Student: {q}\n"
        f"Options:\n{options_text}\n"
        f"Professor: The correct answer is {ans}. "
        f"Explanation: {meta if meta else 'This involves physiological and pharmacological reasoning.'}"
    )
    return {"text": text}


print("ğŸ“˜ Chargement de MedQA (USMLE)...")
MEDQA_PATH = os.path.join(RAW_DIR, "med_qa/data_clean/data_clean/questions/US/train.jsonl")
medqa = load_dataset("json", data_files=MEDQA_PATH)["train"].map(format_medqa)
print(f"âœ… MedQA formatÃ© : {len(medqa)} exemples\n")


# ==========================================================
# 2ï¸âƒ£ Textbooks anglais
# ==========================================================
print("ğŸ“š Chargement des textbooks anglais...")
TEXTBOOK_DIR = os.path.join(RAW_DIR, "med_qa/data_clean/data_clean/textbooks/en")
text_data = []

for filename in os.listdir(TEXTBOOK_DIR):
    if filename.endswith(".txt"):
        path = os.path.join(TEXTBOOK_DIR, filename)
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read().strip()
            if len(content) > MIN_TEXT_LEN:
                title = os.path.splitext(filename)[0].replace("_", " ").title()
                text_data.append({
                    "text": f"Student: Can you explain the topic of {title}?\nProfessor: {content}"
                })

textbooks = Dataset.from_list(text_data)
print(f"âœ… Textbooks formatÃ©s : {len(textbooks)} exemples\n")


# ==========================================================
# 3ï¸âƒ£ MedDialog (anglais)
# ==========================================================
print("ğŸ’¬ Chargement de MedDialog...")
def format_meddialog(example):
    desc = example.get("description", "")
    utts = example.get("utterances", [])
    dialogue = " ".join(utts).replace("patient:", "student:").replace("doctor:", "professor:")
    return {"text": f"Case: {desc}\n{dialogue}"}

MEDDIALOG_PATH = os.path.join(BASE_DIR, "processed/english-train.json")
meddialog = load_dataset("json", data_files=MEDDIALOG_PATH)["train"].map(format_meddialog)
print(f"âœ… MedDialog formatÃ© : {len(meddialog)} exemples\n")


# ==========================================================
# 4ï¸âƒ£ Fusion + Nettoyage
# ==========================================================
print("ğŸ§© Fusion de tous les datasets...")
combined = concatenate_datasets([medqa, textbooks, meddialog])
print(f"âœ… Total initial : {len(combined)} exemples combinÃ©s")

print("ğŸ§¹ Nettoyage des exemples trop courts...")
filtered = combined.filter(lambda x: len(x["text"]) > MIN_TEXT_LEN)
print(f"âœ… {len(filtered)} exemples conservÃ©s aprÃ¨s nettoyage\n")


# ==========================================================
# 5ï¸âƒ£ Split en train / eval
# ==========================================================
print("âœ‚ï¸ Split en train / validation (90/10)...")
filtered = filtered.shuffle(seed=42)
split_idx = int(len(filtered) * TRAIN_RATIO)
train_dataset = filtered.select(range(split_idx))
eval_dataset = filtered.select(range(split_idx, len(filtered)))

print(f"âœ… Train : {len(train_dataset)} exemples")
print(f"âœ… Eval  : {len(eval_dataset)} exemples\n")


# ==========================================================
# 6ï¸âƒ£ Sauvegarde JSONL (format MLX)
# ==========================================================
def save_jsonl(dataset, path):
    with open(path, "w", encoding="utf-8") as f:
        for ex in dataset:
            text = ex["text"].strip().replace("\n\n", "\n")
            f.write(json.dumps({"text": text}, ensure_ascii=False) + "\n")

TRAIN_PATH = os.path.join(SAVE_DIR, "train.jsonl")
VALID_PATH = os.path.join(SAVE_DIR, "valid.jsonl")

print("ğŸ’¾ Sauvegarde des fichiers...")
save_jsonl(train_dataset, TRAIN_PATH)
save_jsonl(eval_dataset, VALID_PATH)

print(f"âœ… Train : {TRAIN_PATH}")
print(f"âœ… valid  : {VALID_PATH}\n")

# ==========================================================
# 7ï¸âƒ£ VÃ©rification rapide du contenu
# ==========================================================
print("ğŸ” Exemples alÃ©atoires :\n")
for i in random.sample(range(len(train_dataset)), min(3, len(train_dataset))):
    print(f"--- Exemple {i} ---")
    print(train_dataset[i]["text"][:500], "...\n")

print("ğŸ‰ Dataset MLX complet et prÃªt Ã  l'emploi !")
