"""
evaluate_model.py
-----------------
Ã‰value un modÃ¨le GPT2-like fine-tunÃ© :
- Calcule la perplexitÃ©
- GÃ©nÃ¨re quelques exemples mÃ©dicaux
- Sauvegarde un rapport dâ€™Ã©valuation (.txt)
- Logge les rÃ©sultats dans MLflow
"""

import os
import math
import torch
from transformers import (
    GPT2LMHeadModel,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    pipeline
)
from datasets import load_from_disk
import mlflow

# ======================
# âš™ï¸ 1. Configuration
# ======================
MODEL_PATH = "training/models/checkpoints/checkpoint-9633"
DATA_PATH = "training/data/processed"
REPORTS_DIR = "training/reports"

DEVICE = "mps" if torch.backends.mps.is_available() else \
         "cuda" if torch.cuda.is_available() else "cpu"

os.makedirs(REPORTS_DIR, exist_ok=True)
print(f"âœ… Utilisation du device : {DEVICE}")

# ======================
# ğŸ“¦ 2. Chargement du modÃ¨le et tokenizer
# ======================
print("ğŸ“¦ Chargement du modÃ¨le...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to(DEVICE)

# ======================
# ğŸ“š 3. Chargement du dataset tokenisÃ©
# ======================
print("ğŸ“¥ Chargement du dataset pour Ã©valuation...")
dataset = load_from_disk(DATA_PATH)
subset = dataset.select(range(2000)) if len(dataset) > 2000 else dataset

# ğŸ§  Ajout des labels nÃ©cessaires pour la perplexitÃ©
if "labels" not in subset.column_names:
    subset = subset.map(lambda ex: {"labels": ex["input_ids"]})

# ======================
# âš™ï¸ 4. Configuration du Trainer pour Ã©valuation
# ======================
args = TrainingArguments(
    output_dir="training/models/eval_output",
    per_device_eval_batch_size=1,
    dataloader_drop_last=True,
    report_to=[],
)

trainer = Trainer(model=model, args=args, eval_dataset=subset)

# ======================
# ğŸ“Š 5. Calcul de la perplexitÃ©
# ======================
print("ğŸ“Š Calcul de la perplexitÃ©...")
eval_results = trainer.evaluate()
print("ğŸ“Š RÃ©sultats bruts :", eval_results)

loss_key = "eval_loss" if "eval_loss" in eval_results else "loss" if "loss" in eval_results else None

if loss_key:
    perplexity = math.exp(eval_results[loss_key])
    print(f"âœ… PerplexitÃ© : {perplexity:.2f}")
else:
    print("âš ï¸ Impossible de calculer la perplexitÃ© : aucune perte trouvÃ©e.")
    perplexity = None

# ======================
# ğŸ’¬ 6. GÃ©nÃ©ration de texte mÃ©dical
# ======================
print("\nğŸ©º Exemples de gÃ©nÃ©ration :")
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0 if DEVICE != "cpu" else -1
)

prompts = [
    "Patient: I have a persistent cough and chest pain. What could this be?\nDoctor:",
    "Question: What is the treatment for type 2 diabetes in obese patients?\nAnswer:",
    "Patient: My throat hurts and I feel feverish for 3 days. What should I do?\nDoctor:",
]

generated_texts = []
for prompt in prompts:
    print(f"\nğŸ§© Prompt: {prompt}\n---")
    result = generator(prompt, max_new_tokens=120, temperature=0.7, top_p=0.95, do_sample=True)
    output = result[0]["generated_text"]
    print(output)
    generated_texts.append((prompt, output))

# ======================
# ğŸ§¾ 7. CrÃ©ation dâ€™un rapport dâ€™Ã©valuation
# ======================
report_path = os.path.join(REPORTS_DIR, "evaluation_report.txt")
with open(report_path, "w") as f:
    f.write("=== Ã‰VALUATION DU MODÃˆLE MÃ‰DICAL ===\n\n")
    if perplexity:
        f.write(f"PerplexitÃ© : {perplexity:.2f}\n\n")
    else:
        f.write("PerplexitÃ© : non calculÃ©e\n\n")

    f.write("=== Exemples de gÃ©nÃ©ration ===\n")
    for i, (prompt, gen) in enumerate(generated_texts, 1):
        f.write(f"\n--- Exemple {i} ---\n")
        f.write(f"Prompt:\n{prompt}\n\nRÃ©ponse gÃ©nÃ©rÃ©e:\n{gen}\n")

print(f"\nğŸ“„ Rapport dâ€™Ã©valuation sauvegardÃ© : {report_path}")

# ======================
# ğŸ“ˆ 8. Log des rÃ©sultats dans MLflow
# ======================
print("\nğŸ“ˆ Log des rÃ©sultats dans MLflow...")
mlflow.set_experiment("Medical-LLM")
with mlflow.start_run(run_name="Evaluation_Run") as run:
    if perplexity:
        mlflow.log_metric("perplexity", perplexity)
    mlflow.log_artifact(report_path)
    mlflow.log_artifact(f"{MODEL_PATH}/config.json")
    mlflow.log_artifact(f"{MODEL_PATH}/pytorch_model.bin" if os.path.exists(f"{MODEL_PATH}/pytorch_model.bin") else f"{MODEL_PATH}/model.safetensors")

print("âœ… Ã‰valuation terminÃ©e et loggÃ©e dans MLflow !")
